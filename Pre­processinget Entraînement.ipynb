{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle as pickle \n",
    "with open('data_with_coord_29_04_21', \"rb\") as fh:\n",
    "    df = pickle.load(fh)\n",
    "df = df[df.dist_euclidian<150000]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "import matplotlib\n",
    "matplotlib.use('nbagg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "subset = df.sample(50000)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "ax.plot(subset.dist_euclidian, subset.AttendanceTimeSeconds, '.', markersize=3)\n",
    "ax.set_xlabel('Distance euclidienne (m)', fontsize=20)\n",
    "ax.set_ylabel('Temps (s)', fontsize=20)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.tick_params(axis='both', which='major', labelsize=15)\n",
    "plt.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset['velocity'] = subset.dist_euclidian/subset.AttendanceTimeSeconds\n",
    "print('la vélocité minimale est de ', subset.velocity.min(),'m/s, et maximale est de ', subset.velocity.max(), 'ms')\n",
    "subset['kmh'] = (subset.velocity*3600)/1000\n",
    "print('Ce qui correspond à une vitesse de ', subset.kmh.min(),'Km/H, et maximale est de ', subset.kmh.max(), 'Km/H')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On peut donc constater que dans les données doivent encore se trouver quelques anomalies :\n",
    "- Des interventions très proches de la station, mais pour lesquelles les pompiers ont pris plusieurs minutes,\n",
    "- Des interventions lointaines, mais pour lesquelles les pompiers ont pu recourir à des véhicules plus rapides (Hélicoptères, formule 1, téléporteur, ou encore Sleipnir, fils de Loki, Destrier d'Odin, le cheval à 8 jambes capable de survoler les mers)\n",
    "- Des interventions différées (relèves, excercices, effort prolongé de lutte contre le feu...)\n",
    "\n",
    "### Comme nous souhaitons nous focaliser sur les évenements necessitant une intervention classique (véhicules), on peut appliquer quelques filtres supplémentaires :\n",
    "- Temps d'intervention > 0s\n",
    "- Distance euclidienne > 100m\n",
    "- Année > 2016 (cf analyse temps de réponse du rapport)\n",
    "- velocity < 50 (180 km/h) & velocity > 7 (25 km/h) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = df.copy()\n",
    "subset = subset[(df.AttendanceTimeSeconds >0) & (subset.dist_euclidian>100)]\n",
    "subset['velocity'] = subset.dist_euclidian / subset.AttendanceTimeSeconds\n",
    "subset = subset[(subset.CalYear>2016) & (subset.velocity <50) & (subset.velocity > 7)]\n",
    "print(\"la base de données est donc réduite de :\", round(100-(subset.shape[0]/df.shape[0])*100, 2), '% et compte maintenant ', subset.shape[0],' entrées')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "ax.plot(subset.dist_euclidian, subset.AttendanceTimeSeconds, '.', markersize=1)\n",
    "ax.set_xlabel('Distance euclidienne (m)', fontsize=20)\n",
    "ax.set_ylabel('Temps (s)', fontsize=20)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.tick_params(axis='both', which='major', labelsize=15)\n",
    "plt.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# La distribution des données ne semble pas s'offrir à l'utilisation d'une régression linéaire. \n",
    "### Nous suggérons qu'elles dépendent de trop de facteurs différents pour offrir une prédiction satisfaisante sans davantage de détails logistiques (absent de la base de données)\n",
    "### Nous avons alors imaginé de produire des informations pertinentes et descripives, en fonction d'un lieu d'intervention.\n",
    "### Il nous faut donc déterminer pour chaque point de la carte, la station qui devrait intervenir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Si on observe les points d'interventions par station :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "cmap=plt.cm.get_cmap(plt.cm.rainbow)\n",
    "max_s = len(subset.DeployedFromStation_Name.unique())\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "for i,s in enumerate(subset.DeployedFromStation_Name.unique()):\n",
    "    ax.plot(subset[subset.DeployedFromStation_Name==s].x_utm, \n",
    "            subset[subset.DeployedFromStation_Name==s].y_utm,\n",
    "           '.',\n",
    "           color=cmap(i/max_s),\n",
    "           markersize=1)\n",
    "#sns.scatterplot(x='x_utm', y = 'y_utm', hue='DeployedFromStation_Name',legend=False,data=subset, sizes=0.2, ax=ax);\n",
    "ax.set_xlabel('X', fontsize=20)\n",
    "ax.set_ylabel('Y', fontsize=20)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.tick_params(labelsize=15)\n",
    "plt.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Les données semblent difficiles à distinguer, et un kmeans appliqué directement risque de générer beaucoup d'erreurs\n",
    "### Nous avons eu l'idée de renforcer l'apprentissage en augmentant la distinctivité entre les stations\n",
    "### C'est à dire, de classer des points intermédiaire sur la distance station-intervention, plutôt que la destination elle même\n",
    "Exemple lorsqu'on prend la moitié de cette distance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset['middle_x'] = subset.o_x_utm + (1/2)*(subset.x_utm-subset.o_x_utm)\n",
    "subset['middle_y'] = subset.o_y_utm + (1/2)*(subset.y_utm-subset.o_y_utm)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "for i,s in enumerate(subset.DeployedFromStation_Name.unique()):\n",
    "    ax.plot(subset[subset.DeployedFromStation_Name==s].middle_x, \n",
    "            subset[subset.DeployedFromStation_Name==s].middle_y,\n",
    "           '.',\n",
    "           color=cmap(i/max_s),\n",
    "           markersize=1)\n",
    "ax.set_xlabel('X', fontsize=20)\n",
    "ax.set_ylabel('Y', fontsize=20)\n",
    "ax.tick_params(labelsize=15)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "plt.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Les points semblent plus propices à la classification par kmeans.\n",
    "### On peut donc généraliser l'utilisation des distances pour trouver un ratio optimal (qui maximise la classification):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = []\n",
    "for i in subset.DeployedFromStation_Name.unique():\n",
    "    # pour chaque station, on récupère les coordonnées x y, ainsi que le nom\n",
    "    x, y = subset[subset.DeployedFromStation_Name==i].iloc[0, :].o_x_utm, subset[subset.DeployedFromStation_Name==i].iloc[0, :].o_y_utm\n",
    "    centers.append((x, y, i))\n",
    "centers = np.array(centers)\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# sample car les données sont nombreuses et c'est exagéré pour le modèle\n",
    "# subset = subset.sample(50000)\n",
    "def learn_test(a, b, subset):\n",
    "    # le modèle marche mieux avec des morceau de segment, contrôle de la portion la plus performante à faire\n",
    "    subset['middle_x'] = subset.o_x_utm + (a/b)*(subset.x_utm-subset.o_x_utm)\n",
    "    subset['middle_y'] = subset.o_y_utm + (a/b)*(subset.y_utm-subset.o_y_utm)\n",
    "    \n",
    "    index = np.arange(subset.shape[0])\n",
    "    subset = subset.set_index(index)\n",
    "    np.random.shuffle(index)\n",
    "    ratio_test = int(0.2*subset.shape[0])\n",
    "    # on calcul le poids de chaque station (nombre d'intervention par station / nombre total d'intervention)\n",
    "    w = np.array(subset.loc[index[:-ratio_test], ['DeployedFromStation_Name']].value_counts().tolist()) / (subset.shape[0]-ratio_test)\n",
    "\n",
    "    # on initialise la mixture gaussienne avec autant de centre que de station, initialisé sur les stations,\n",
    "    gm = KMeans(n_clusters=centers.shape[0], init=centers[:, :2], n_init=1, random_state=1337)\n",
    "\n",
    "    # fit et sub pour le test\n",
    "    gm.fit(subset.loc[index[:-ratio_test], ['middle_x','middle_y']])\n",
    "    print(a, b, \" fit ok\")\n",
    "    y = subset.loc[index[-ratio_test:], ['DeployedFromStation_Name']]\n",
    "    \n",
    "    y_pred = gm.predict(subset.loc[index[-ratio_test:], ['x_utm','y_utm']])\n",
    "\n",
    "    print(a, b, \" collecte ok\")\n",
    "    success = 0\n",
    "    predicted = centers[y_pred, -1]\n",
    "    success = np.where(y.DeployedFromStation_Name==predicted)[0].shape[0]/y.shape[0]\n",
    "    return success\n",
    "\n",
    "results_km = []\n",
    "for a in range(11):\n",
    "    results_km.append(learn_test(a, 10, subset))\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "ax.plot(results_km)\n",
    "ax.set_xticks(np.arange(11))\n",
    "ax.set_xticklabels((np.arange(11)*0.1).round(2))\n",
    "ax.set_xlabel('Ratio du segment station-incident', fontsize=25)\n",
    "ax.set_ylabel('Ratio de réussite', fontsize=25)\n",
    "ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# La performance moyenne avec la méthode Kmeans n'est pas fantastique (max 63%). Nous pensons que la cause principale de ce score est le fait que les stations ont fréquemment pu intervenir sur des secteurs communs\n",
    "\n",
    "### Une solution pertinente serait, pour chaque lieu d'incident, de déterminer quels sont les stations qui sont susceptibles d'y intervenir. \n",
    "Bien sûr, on pourrait simplement utiliser la distance, mais il serait difficile de fixer le seuil : un incident en campagne pourrait faire intervenir 2-3 stations situées à 10-15 km, mais un incident en ville pourrait faire intervenir 4-5 stations situées à 3 km. Gérer ces spécificités individuellement pourrait s'avérer plus compliqué qu'appliquer une méthode de ML.\n",
    "\n",
    "### Cependant, les Kmeans ne renvoies pas de probabilité associée à la classe. Nous choisissons donc d'utiliser les Mixtures Gaussiennes, une généralisation de l'algorithme des Kmeans, mais qui leur associe une covariance. Cette covariance peut capturer les caractéristiques de chaque station, en fonction de leur historique d'intervention.\n",
    "On pourra alors, pour chaque lieu d'incident, déterminer quelles station sont susceptibles d'intervenir. On pourra également vérifier si parmis les stations candidates, la station du jeu de donnée est bien présente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture as GM\n",
    "def learn_test(a, b, subset):\n",
    "    # le modèle marche mieux avec des morceau de segment, contrôle de la portion la plus performante à faire\n",
    "    subset['middle_x'] = subset.o_x_utm + (a/b)*(subset.x_utm-subset.o_x_utm)\n",
    "    subset['middle_y'] = subset.o_y_utm + (a/b)*(subset.y_utm-subset.o_y_utm)\n",
    "    \n",
    "    index = np.arange(subset.shape[0])\n",
    "    subset = subset.set_index(index)\n",
    "    np.random.shuffle(index)\n",
    "    ratio_test = int(0.2*subset.shape[0])\n",
    "    # on calcul le poids de chaque station (nombre d'intervention par station / nombre total d'intervention)\n",
    "    w = np.array(subset.loc[index[:-ratio_test], ['DeployedFromStation_Name']].value_counts().tolist()) / (subset.shape[0]-ratio_test)\n",
    "\n",
    "    # on initialise la mixture gaussienne avec autant de centre que de station, initialisé sur les stations, pondéré par le taux d'intervention\n",
    "    gm = GM(n_components=centers.shape[0], means_init=centers[:, :2], n_init=1, random_state=1337 )\n",
    "\n",
    "    # fit et sub pour le test\n",
    "    gm.fit(subset.loc[index[:-ratio_test], ['middle_x','middle_y']])\n",
    "    print(a, b, \" fit ok\")\n",
    "    y = subset.loc[index[-ratio_test:], ['DeployedFromStation_Name']]\n",
    "    gm.means_=centers[:, :2].astype(float) #remets les stations aux centres des mixtures\n",
    "    y_pred = gm.predict_proba(subset.loc[index[-ratio_test:], ['x_utm','y_utm']])\n",
    "    y_pred = np.where(y_pred>0.0001)\n",
    "    \n",
    "    count_prob = []\n",
    "    for i in np.unique(y_pred[0]):\n",
    "        ind = y_pred[0]\n",
    "        count_prob.append(y_pred[1][ind==i])\n",
    "    print(a, b, \" collecte ok\")\n",
    "    success = 0\n",
    "    for i, c in enumerate(count_prob):\n",
    "        if np.isin(y.iloc[i, 0], centers[c, -1]):\n",
    "            success += 1/(len(count_prob))\n",
    "\n",
    "    return success\n",
    "results_gm = []\n",
    "for a in range(11):\n",
    "    results_gm.append(learn_test(a, 10, subset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(results_gm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avec cette approche, on dépasse les 95% de succès. On observe également que le modèle est plus performant lorsqu'il est entraîné avec l'intégralité de la distance station-incident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "ax.plot(results_km, color='b', label='Kmeans')\n",
    "ax.plot(results_gm, color='r', label='GMM')\n",
    "ax.set_ylim([0, 1])\n",
    "ax.set_xticks(np.arange(11))\n",
    "ax.set_xticklabels((np.arange(11)*0.1).round(2))\n",
    "ax.set_xlabel('Ratio du segment station-incident', fontsize=20)\n",
    "ax.set_ylabel('Ratio de réussite', fontsize=20)\n",
    "ax.tick_params(axis='both', which='major', labelsize=15)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "plt.legend(fontsize=20)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample avec trajets\n",
    "\n",
    "subset['middle_x'] = subset.o_x_utm + (10/10)*(subset.x_utm-subset.o_x_utm)\n",
    "subset['middle_y'] = subset.o_y_utm + (10/10)*(subset.y_utm-subset.o_y_utm)\n",
    "print(subset.shape[0])\n",
    "\n",
    "# on calcul le poids de chaque station (nombre d'intervention par station / nombre total d'intervention)\n",
    "w = np.array(subset.loc[:, ['DeployedFromStation_Name']].value_counts().tolist()) / (subset.shape[0])\n",
    "\n",
    "# on initialise la mixture gaussienne avec autant de centre que de station, initialisé sur les stations, pondéré par le taux d'intervention\n",
    "gm = GM(n_components=centers.shape[0], means_init =centers[:, :2], n_init=1, weights_init = w)\n",
    "# fit et sub pour le test\n",
    "gm.fit(subset.loc[:, ['middle_x','middle_y']])\n",
    "gm.means_=centers[:, :2].astype(float) #remets les stations aux centres des mixtures\n",
    "gmm_name = 'gmm_29_04_21'\n",
    "np.save(gmm_name + '_weights', gm.weights_, allow_pickle=False)\n",
    "np.save(gmm_name + '_means', gm.means_, allow_pickle=False)\n",
    "np.save(gmm_name + '_covariances', gm.covariances_, allow_pickle=False)\n",
    "print(gm.means_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import compute_path as cp\n",
    "from sklearn import mixture\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import  push_notebook,output_notebook, show\n",
    "from bokeh.models import ColumnDataSource, LabelSet,Ellipse, Column\n",
    "from bokeh.models import PreText, CustomJS,HTMLTemplateFormatter\n",
    "from bokeh.models import TapTool, Div, DataTable, TableColumn, TextInput\n",
    "from bokeh.tile_providers import get_provider\n",
    "from bokeh.models.tools import HoverTool\n",
    "from bokeh.layouts import row, layout, column, widgetbox\n",
    "from streamlit_bokeh_events import streamlit_bokeh_events\n",
    "\n",
    "import pickle as pickle \n",
    "\n",
    "station_coord = pd.read_csv('Station_coord.csv')\n",
    "station_coord = station_coord.rename(columns={'long':'lon'})\n",
    "subset = subset.rename(columns={'d_long':'lon', 'd_lat':'lat'})\n",
    "\n",
    "def make_ellipses(p, gmm_name, station_coord):\n",
    "    means = np.load(gmm_name + '_means.npy')\n",
    "    covar = np.load(gmm_name + '_covariances.npy')\n",
    "    gmm = mixture.GaussianMixture(n_components = len(means), covariance_type='full')\n",
    "    gmm.precisions_cholesky_ = np.linalg.cholesky(np.linalg.inv(covar))\n",
    "    gmm.weights_ = np.load(gmm_name + '_weights.npy')\n",
    "    gmm.means_ = means\n",
    "    gmm.covariances_ = covar\n",
    "    vs, angles = np.zeros((len(gmm.covariances_),2)), np.zeros((len(gmm.covariances_)))\n",
    "    for n in range(len(gmm.covariances_)):\n",
    "        covariances = gmm.covariances_[n][:2, :2]\n",
    "        v, w = np.linalg.eigh(covariances)\n",
    "        u = w[0] / np.linalg.norm(w[0])\n",
    "        angle = np.arctan2(u[1], u[0])\n",
    "        angle = 180 * angle / np.pi  # convert to degrees\n",
    "        angles[n] = angle\n",
    "        v = 2. * np.sqrt(2.) * np.sqrt(v)\n",
    "        vs[n, :] = v\n",
    "\n",
    "    stations = pd.DataFrame(gmm.means_, columns=['x','y'])\n",
    "    stations['width'] = vs[:, 0]\n",
    "    stations['height'] = vs[:, 1]\n",
    "    stations['angle'] = angles+180\n",
    "    stations['names'] = subset.DeployedFromStation_Name.unique()\n",
    "    source = ColumnDataSource(stations)\n",
    "    ellipses = p.ellipse(x='x', y='y', width='width', height='height', angle='angle', fill_color=\"#cab2d6\", fill_alpha=0.2, source=source)\n",
    "    return ellipses, station, source\n",
    "\n",
    "tuile = get_provider('CARTODBPOSITRON_RETINA')\n",
    "tuile2 = get_provider('CARTODBPOSITRON_RETINA')\n",
    "lon, lat = cp.wgs84_to_web_mercator(station_coord.lat, station_coord.lon)\n",
    "station_coord['x'] = lon\n",
    "station_coord['y'] = lat\n",
    "source = ColumnDataSource(station_coord)\n",
    "# On récupère les valeurs mini/maxi des coordonnées,\n",
    "# on rajoute 10km pour avoir une carte centrée\n",
    "xutm_min, xutm_max, yutm_min, yutm_max = station_coord.x.min()-10000, station_coord.x.max()+10000, \\\n",
    "                                         station_coord.y.min()-10000, station_coord.y.max()+10000\n",
    "\n",
    "x_range, y_range, x_axis_type, y_axis_type = (xutm_min, xutm_max), (yutm_min, yutm_max), 'mercator', 'mercator'\n",
    "\n",
    "p = figure(x_range=x_range, y_range=y_range, x_axis_type=x_axis_type, y_axis_type=y_axis_type,\n",
    "           plot_width=1200, plot_height=1200)\n",
    "p.add_tile(tuile)\n",
    "\n",
    "station = p.circle(x='x',y='y', size=5, fill_color='blue', source=source)\n",
    "\n",
    "ellipses, station_df, source_ell = make_ellipses(p, 'gmm_29_04_21', station_coord)\n",
    "p.xaxis.major_label_text_font_size = '25px'\n",
    "p.xaxis.axis_label = \"Lon\"\n",
    "p.xaxis.axis_label_text_font_size = '25px'\n",
    "\n",
    "p.yaxis.major_label_text_font_size = '25px'\n",
    "p.yaxis.axis_label = \"Lat\"\n",
    "p.yaxis.axis_label_text_font_size = '25px'\n",
    "show(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
